{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kimyw/anaconda3/envs/FLLM/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import datetime\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from timm.data import Mixup\n",
    "from timm.models import create_model\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
    "from timm.scheduler import create_scheduler\n",
    "from timm.optim import create_optimizer\n",
    "from timm.utils import NativeScaler, get_state_dict, ModelEma\n",
    "\n",
    "import models.vit\n",
    "import models.vit_llama\n",
    "from datasets import build_dataset, build_transform\n",
    "from engine import train_one_epoch, evaluate\n",
    "from utils import DistillationLoss\n",
    "from utils import RASampler\n",
    "import utils\n",
    "from optimizer_utils import my_create_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"vit_small_patch16_224\"\n",
    "data_path = \"./\"\n",
    "eval = 0\n",
    "resume = \"./image_classification/checkpoints/vit_small_patch16_224/checkpoint.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: DeiT training and evaluation script [-h] [--exp_name EXP_NAME]\n",
      "                                           [--batch-size BATCH_SIZE]\n",
      "                                           [--epochs EPOCHS] [--wandb]\n",
      "                                           [--model MODEL]\n",
      "                                           [--input-size INPUT_SIZE]\n",
      "                                           [--drop PCT] [--drop-path PCT]\n",
      "                                           [--model-ema] [--no-model-ema]\n",
      "                                           [--model-ema-decay MODEL_EMA_DECAY]\n",
      "                                           [--model-ema-force-cpu]\n",
      "                                           [--llama_path LLAMA_PATH]\n",
      "                                           [--param_filter PARAM_FILTER]\n",
      "                                           [--use_patch_aug] [--opt OPTIMIZER]\n",
      "                                           [--opt-eps EPSILON]\n",
      "                                           [--opt-betas BETA [BETA ...]]\n",
      "                                           [--clip-grad NORM] [--momentum M]\n",
      "                                           [--weight-decay WEIGHT_DECAY]\n",
      "                                           [--sched SCHEDULER] [--lr LR]\n",
      "                                           [--lr-noise pct, pct [pct, pct ...]]\n",
      "                                           [--lr-noise-pct PERCENT]\n",
      "                                           [--lr-noise-std STDDEV]\n",
      "                                           [--warmup-lr LR] [--min-lr LR]\n",
      "                                           [--decay-epochs N]\n",
      "                                           [--warmup-epochs N]\n",
      "                                           [--cooldown-epochs N]\n",
      "                                           [--patience-epochs N]\n",
      "                                           [--decay-rate RATE]\n",
      "                                           [--color-jitter PCT] [--aa NAME]\n",
      "                                           [--smoothing SMOOTHING]\n",
      "                                           [--train-interpolation TRAIN_INTERPOLATION]\n",
      "                                           [--repeated-aug]\n",
      "                                           [--no-repeated-aug] [--reprob PCT]\n",
      "                                           [--remode REMODE]\n",
      "                                           [--recount RECOUNT] [--resplit]\n",
      "                                           [--mixup MIXUP] [--cutmix CUTMIX]\n",
      "                                           [--cutmix-minmax CUTMIX_MINMAX [CUTMIX_MINMAX ...]]\n",
      "                                           [--mixup-prob MIXUP_PROB]\n",
      "                                           [--mixup-switch-prob MIXUP_SWITCH_PROB]\n",
      "                                           [--mixup-mode MIXUP_MODE]\n",
      "                                           [--teacher-model MODEL]\n",
      "                                           [--teacher-path TEACHER_PATH]\n",
      "                                           [--distillation-type {none,soft,hard}]\n",
      "                                           [--distillation-alpha DISTILLATION_ALPHA]\n",
      "                                           [--distillation-tau DISTILLATION_TAU]\n",
      "                                           [--finetune FINETUNE]\n",
      "                                           [--pretrained]\n",
      "                                           [--data-path DATA_PATH]\n",
      "                                           [--data-set {CIFAR,IMNET,INAT,INAT19}]\n",
      "                                           [--data_type {tar,folder}]\n",
      "                                           [--inat-category {kingdom,phylum,class,order,supercategory,family,genus,name}]\n",
      "                                           [--output_dir OUTPUT_DIR]\n",
      "                                           [--device DEVICE] [--seed SEED]\n",
      "                                           [--resume RESUME] [--start_epoch N]\n",
      "                                           [--eval EVAL] [--inc_path INC_PATH]\n",
      "                                           [--ina_path INA_PATH]\n",
      "                                           [--inr_path INR_PATH]\n",
      "                                           [--insk_path INSK_PATH]\n",
      "                                           [--fgsm_test] [--pgd_test]\n",
      "                                           [--dist-eval]\n",
      "                                           [--num_workers NUM_WORKERS]\n",
      "                                           [--pin-mem] [--no-pin-mem]\n",
      "                                           [--local_rank LOCAL_RANK]\n",
      "                                           [--world_size WORLD_SIZE]\n",
      "                                           [--dist_url DIST_URL]\n",
      "                                           [--test_every TEST_EVERY]\n",
      "DeiT training and evaluation script: error: ambiguous option: --f=/home/kimyw/.local/share/jupyter/runtime/kernel-v2-3159g1dsLPFiirzF.json could match --finetune, --fgsm_test\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser('RVT training and evaluation script', add_help=False)\n",
    "    parser.add_argument('--exp_name', default='debug', type=str)\n",
    "    parser.add_argument('--batch-size', default=128, type=int)\n",
    "    parser.add_argument('--epochs', default=300, type=int)\n",
    "\n",
    "    # Logging parameters\n",
    "    parser.add_argument('--wandb', action='store_true', default=False)\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--model', default='vit_small_patch16_224', type=str, metavar='MODEL',\n",
    "                        help='Name of model to train')\n",
    "    parser.add_argument('--input-size', default=224, type=int, help='images input size')\n",
    "\n",
    "    parser.add_argument('--drop', type=float, default=0.0, metavar='PCT',\n",
    "                        help='Dropout rate (default: 0.)')\n",
    "    parser.add_argument('--drop-path', type=float, default=0.1, metavar='PCT',\n",
    "                        help='Drop path rate (default: 0.1)')\n",
    "\n",
    "    parser.add_argument('--model-ema', action='store_true')\n",
    "    parser.add_argument('--no-model-ema', action='store_false', dest='model_ema')\n",
    "    parser.set_defaults(model_ema=True)\n",
    "    parser.add_argument('--model-ema-decay', type=float, default=0.99996, help='')\n",
    "    parser.add_argument('--model-ema-force-cpu', action='store_true', default=False, help='')\n",
    "\n",
    "    # LLama layers\n",
    "    parser.add_argument('--llama_path', type=str, default='/tmp/7B/')\n",
    "\n",
    "    # optimizer\n",
    "    parser.add_argument('--param_filter', type=str, default='llama.layers')\n",
    "\n",
    "    # RVT params\n",
    "    parser.add_argument('--use_patch_aug', action='store_true')\n",
    "\n",
    "    # Optimizer parameters\n",
    "    parser.add_argument('--opt', default='adamw', type=str, metavar='OPTIMIZER',\n",
    "                        help='Optimizer (default: \"adamw\"')\n",
    "    parser.add_argument('--opt-eps', default=1e-8, type=float, metavar='EPSILON',\n",
    "                        help='Optimizer Epsilon (default: 1e-8)')\n",
    "    parser.add_argument('--opt-betas', default=None, type=float, nargs='+', metavar='BETA',\n",
    "                        help='Optimizer Betas (default: None, use opt default)')\n",
    "    parser.add_argument('--clip-grad', type=float, default=None, metavar='NORM',\n",
    "                        help='Clip gradient norm (default: None, no clipping)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "                        help='SGD momentum (default: 0.9)')\n",
    "    parser.add_argument('--weight-decay', type=float, default=0.05,\n",
    "                        help='weight decay (default: 0.05)')\n",
    "    # Learning rate schedule parameters\n",
    "    parser.add_argument('--sched', default='cosine', type=str, metavar='SCHEDULER',\n",
    "                        help='LR scheduler (default: \"cosine\"')\n",
    "    parser.add_argument('--lr', type=float, default=5e-4, metavar='LR',\n",
    "                        help='learning rate (default: 5e-4)')\n",
    "    parser.add_argument('--lr-noise', type=float, nargs='+', default=None, metavar='pct, pct',\n",
    "                        help='learning rate noise on/off epoch percentages')\n",
    "    parser.add_argument('--lr-noise-pct', type=float, default=0.67, metavar='PERCENT',\n",
    "                        help='learning rate noise limit percent (default: 0.67)')\n",
    "    parser.add_argument('--lr-noise-std', type=float, default=1.0, metavar='STDDEV',\n",
    "                        help='learning rate noise std-dev (default: 1.0)')\n",
    "    parser.add_argument('--warmup-lr', type=float, default=1e-6, metavar='LR',\n",
    "                        help='warmup learning rate (default: 1e-6)')\n",
    "    parser.add_argument('--min-lr', type=float, default=1e-5, metavar='LR',\n",
    "                        help='lower lr bound for cyclic schedulers that hit 0 (1e-5)')\n",
    "\n",
    "    parser.add_argument('--decay-epochs', type=float, default=30, metavar='N',\n",
    "                        help='epoch interval to decay LR')\n",
    "    parser.add_argument('--warmup-epochs', type=int, default=5, metavar='N',\n",
    "                        help='epochs to warmup LR, if scheduler supports')\n",
    "    parser.add_argument('--cooldown-epochs', type=int, default=10, metavar='N',\n",
    "                        help='epochs to cooldown LR at min_lr, after cyclic schedule ends')\n",
    "    parser.add_argument('--patience-epochs', type=int, default=10, metavar='N',\n",
    "                        help='patience epochs for Plateau LR scheduler (default: 10')\n",
    "    parser.add_argument('--decay-rate', '--dr', type=float, default=0.1, metavar='RATE',\n",
    "                        help='LR decay rate (default: 0.1)')\n",
    "\n",
    "    # Augmentation parameters\n",
    "    parser.add_argument('--color-jitter', type=float, default=0.4, metavar='PCT',\n",
    "                        help='Color jitter factor (default: 0.4)')\n",
    "    parser.add_argument('--aa', type=str, default='rand-m9-mstd0.5-inc1', metavar='NAME',\n",
    "                        help='Use AutoAugment policy. \"v0\" or \"original\". \" + \\\n",
    "                             \"(default: rand-m9-mstd0.5-inc1)'),\n",
    "    parser.add_argument('--smoothing', type=float, default=0.1, help='Label smoothing (default: 0.1)')\n",
    "    parser.add_argument('--train-interpolation', type=str, default='bicubic',\n",
    "                        help='Training interpolation (random, bilinear, bicubic default: \"bicubic\")')\n",
    "\n",
    "    parser.add_argument('--repeated-aug', action='store_true')\n",
    "    parser.add_argument('--no-repeated-aug', action='store_false', dest='repeated_aug')\n",
    "    parser.set_defaults(repeated_aug=True)\n",
    "\n",
    "    # * Random Erase params\n",
    "    parser.add_argument('--reprob', type=float, default=0.25, metavar='PCT',\n",
    "                        help='Random erase prob (default: 0.25)')\n",
    "    parser.add_argument('--remode', type=str, default='pixel',\n",
    "                        help='Random erase mode (default: \"pixel\")')\n",
    "    parser.add_argument('--recount', type=int, default=1,\n",
    "                        help='Random erase count (default: 1)')\n",
    "    parser.add_argument('--resplit', action='store_true', default=False,\n",
    "                        help='Do not random erase first (clean) augmentation split')\n",
    "\n",
    "    # * Mixup params\n",
    "    parser.add_argument('--mixup', type=float, default=0.8,\n",
    "                        help='mixup alpha, mixup enabled if > 0. (default: 0.8)')\n",
    "    parser.add_argument('--cutmix', type=float, default=1.0,\n",
    "                        help='cutmix alpha, cutmix enabled if > 0. (default: 1.0)')\n",
    "    parser.add_argument('--cutmix-minmax', type=float, nargs='+', default=None,\n",
    "                        help='cutmix min/max ratio, overrides alpha and enables cutmix if set (default: None)')\n",
    "    parser.add_argument('--mixup-prob', type=float, default=1.0,\n",
    "                        help='Probability of performing mixup or cutmix when either/both is enabled')\n",
    "    parser.add_argument('--mixup-switch-prob', type=float, default=0.5,\n",
    "                        help='Probability of switching to cutmix when both mixup and cutmix enabled')\n",
    "    parser.add_argument('--mixup-mode', type=str, default='batch',\n",
    "                        help='How to apply mixup/cutmix params. Per \"batch\", \"pair\", or \"elem\"')\n",
    "\n",
    "    # Distillation parameters\n",
    "    parser.add_argument('--teacher-model', default='regnety_160', type=str, metavar='MODEL',\n",
    "                        help='Name of teacher model to train (default: \"regnety_160\"')\n",
    "    parser.add_argument('--teacher-path', type=str, default='')\n",
    "    parser.add_argument('--distillation-type', default='none', choices=['none', 'soft', 'hard'], type=str, help=\"\")\n",
    "    parser.add_argument('--distillation-alpha', default=0.5, type=float, help=\"\")\n",
    "    parser.add_argument('--distillation-tau', default=1.0, type=float, help=\"\")\n",
    "\n",
    "    # * Finetuning params\n",
    "    parser.add_argument('--finetune', default='', help='finetune from checkpoint')\n",
    "    parser.add_argument('--pretrained', action='store_true', help='load pretrained model')\n",
    "\n",
    "    # Dataset parameters\n",
    "    parser.add_argument('--data-path', default='./', type=str,\n",
    "                        help='dataset path')\n",
    "    parser.add_argument('--data-set', default='IMNET', choices=['CIFAR', 'IMNET', 'INAT', 'INAT19'],\n",
    "                        type=str, help='Image Net dataset path')\n",
    "    parser.add_argument('--data_type', default='folder', choices=['tar', 'folder'], type=str)\n",
    "    parser.add_argument('--inat-category', default='name',\n",
    "                        choices=['kingdom', 'phylum', 'class', 'order', 'supercategory', 'family', 'genus', 'name'],\n",
    "                        type=str, help='semantic granularity')\n",
    "\n",
    "    parser.add_argument('--output_dir', default='',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=0, type=int)\n",
    "    parser.add_argument('--resume', default='./image_classification/checkpoints/vit_small_patch16_224/checkpoint.pth', help='resume from checkpoint')\n",
    "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "\n",
    "    # eval parameters\n",
    "    # parser.add_argument('--eval', action='store_true', help='Perform evaluation only')\n",
    "    parser.add_argument('--eval', default=True, help='Perform evaluation only')\n",
    "    parser.add_argument('--inc_path', default=None, type=str, help='imagenet-c')\n",
    "    parser.add_argument('--ina_path', default=None, type=str, help='imagenet-a')\n",
    "    parser.add_argument('--inr_path', default=None, type=str, help='imagenet-r')\n",
    "    parser.add_argument('--insk_path', default=None, type=str, help='imagenet-sketch')\n",
    "    parser.add_argument('--fgsm_test', action='store_true', default=False, help='test on FGSM attacker')\n",
    "    parser.add_argument('--pgd_test', action='store_true', default=False, help='test on PGD attacker')\n",
    "\n",
    "    parser.add_argument('--dist-eval', action='store_true', default=False, help='Enabling distributed evaluation')\n",
    "    parser.add_argument('--num_workers', default=10, type=int)\n",
    "    parser.add_argument('--pin-mem', action='store_true',\n",
    "                        help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\n",
    "    parser.add_argument('--no-pin-mem', action='store_false', dest='pin_mem',\n",
    "                        help='')\n",
    "    parser.set_defaults(pin_mem=True)\n",
    "\n",
    "    # distributed training parameters\n",
    "    parser.add_argument(\"--local_rank\", default=0, type=int)\n",
    "    parser.add_argument('--world_size', default=1, type=int,\n",
    "                        help='number of distributed processes')\n",
    "    parser.add_argument('--dist_url', default='env://', help='url used to set up distributed training')\n",
    "\n",
    "    parser.add_argument(\"--test_every\", default=1, type=int)\n",
    "    return parser\n",
    "\n",
    "parser = argparse.ArgumentParser('DeiT training and evaluation script', parents=[get_args_parser()])\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val, _ = build_dataset(is_train=False, args=args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
